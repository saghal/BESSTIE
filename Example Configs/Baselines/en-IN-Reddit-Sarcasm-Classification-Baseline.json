{
    "train_file":"Dataset/Splits/Sarcasm/en-IN/Reddit/Baseline/train_cm.csv",
    "valid_file":"Dataset/Splits/Sarcasm/en-IN/Reddit/Baseline/valid_cm.csv",
    "test_file":"Dataset/Splits/Sarcasm/en-IN/Reddit/Baseline/test_cm.csv",
    "task":"sarcasm",
    "model_name_or_path":"distilbert/distilbert-base-multilingual-cased",
    "type":"SEQ_CLS",
    "trust_remote_code":true,
    "torch_dtype":"auto",
    "attn_implementation":"eager",
    "low_cpu_mem_usage":false,
    "lora":false,
    "quant":false,
    "seed":50,
    "num_labels":2,
    "load_best_model_at_end":true,
    "per_device_train_batch_size":8,
    "per_device_eval_batch_size":8,
    "warmup_steps":500,
    "weight_decay":0.01,
    "learning_rate":2e-5,
    "num_train_epochs":30,
    "eval_strategy":"steps",
    "save_strategy":"steps",
    "save_steps":400,
    "eval_steps":400,
    "logging_steps":100,
    "metric_for_best_model":"eval_loss",
    "greater_is_better":false,
    "result_dir":"Results/",
    "logging_dir":".logs/",
    "output_dir":".output/",
    "cache_dir":".cache/",
    "overwrite_output_dir":true
}