{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Setup and Install\n",
        "!unzip -o BESSTIE.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDKKH66eCz5A",
        "outputId": "865a8e88-d357-4110-cd3b-060c3f3a26f3"
      },
      "id": "hDKKH66eCz5A",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  BESSTIE.zip\n",
            "   creating: Flyer/\n",
            "  inflating: Flyer/besstie-flyer.pdf  \n",
            "  inflating: Flyer/besstie-flyer.pptx  \n",
            "   creating: Notebooks/\n",
            "   creating: Notebooks/Language Probability/\n",
            "  inflating: Notebooks/Language Probability/Language Probability.ipynb  \n",
            "   creating: Notebooks/Mistral Example/\n",
            "  inflating: Notebooks/Mistral Example/cross-variety.ipynb  \n",
            "  inflating: Notebooks/Mistral Example/pre-trained.ipynb  \n",
            "  inflating: Notebooks/Results.ipynb  \n",
            "   creating: Notebooks/Variety Identification - Automatic/\n",
            "  inflating: Notebooks/Variety Identification - Automatic/variety-identification.ipynb  \n",
            "   creating: Notebooks/Variety Identification - Manual/\n",
            "   creating: Notebooks/Variety Identification - Manual/Data/\n",
            "  inflating: Notebooks/Variety Identification - Manual/Data/Combined.csv  \n",
            "  inflating: Notebooks/Variety Identification - Manual/Data/Google.csv  \n",
            "  inflating: Notebooks/Variety Identification - Manual/Data/Reddit.csv  \n",
            "  inflating: Notebooks/Variety Identification - Manual/Manual Variety Identification.ipynb  \n",
            "   creating: Results/\n",
            "   creating: Results/ALBERT/\n",
            "   creating: Results/ALBERT/Sarcasm/\n",
            "   creating: Results/ALBERT/Sarcasm/en-AU/\n",
            "   creating: Results/ALBERT/Sarcasm/en-AU/Reddit/\n",
            "  inflating: Results/ALBERT/Sarcasm/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/ALBERT/Sarcasm/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/ALBERT/Sarcasm/en-IN/\n",
            "   creating: Results/ALBERT/Sarcasm/en-IN/Reddit/\n",
            "  inflating: Results/ALBERT/Sarcasm/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/ALBERT/Sarcasm/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/ALBERT/Sarcasm/en-UK/\n",
            "   creating: Results/ALBERT/Sarcasm/en-UK/Reddit/\n",
            "  inflating: Results/ALBERT/Sarcasm/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/ALBERT/Sarcasm/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/ALBERT/Sentiment/\n",
            "   creating: Results/ALBERT/Sentiment/en-AU/\n",
            "   creating: Results/ALBERT/Sentiment/en-AU/Google/\n",
            "  inflating: Results/ALBERT/Sentiment/en-AU/Google/metric.csv  \n",
            "  inflating: Results/ALBERT/Sentiment/en-AU/Google/prediction.csv  \n",
            "   creating: Results/ALBERT/Sentiment/en-AU/Reddit/\n",
            "  inflating: Results/ALBERT/Sentiment/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/ALBERT/Sentiment/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/ALBERT/Sentiment/en-IN/\n",
            "   creating: Results/ALBERT/Sentiment/en-IN/Google/\n",
            "  inflating: Results/ALBERT/Sentiment/en-IN/Google/metric.csv  \n",
            "  inflating: Results/ALBERT/Sentiment/en-IN/Google/prediction.csv  \n",
            "   creating: Results/ALBERT/Sentiment/en-IN/Reddit/\n",
            "  inflating: Results/ALBERT/Sentiment/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/ALBERT/Sentiment/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/ALBERT/Sentiment/en-UK/\n",
            "   creating: Results/ALBERT/Sentiment/en-UK/Google/\n",
            "  inflating: Results/ALBERT/Sentiment/en-UK/Google/metric.csv  \n",
            "  inflating: Results/ALBERT/Sentiment/en-UK/Google/prediction.csv  \n",
            "   creating: Results/ALBERT/Sentiment/en-UK/Reddit/\n",
            "  inflating: Results/ALBERT/Sentiment/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/ALBERT/Sentiment/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/BERT/\n",
            "   creating: Results/BERT/Sarcasm/\n",
            "   creating: Results/BERT/Sarcasm/en-AU/\n",
            "   creating: Results/BERT/Sarcasm/en-AU/Reddit/\n",
            "  inflating: Results/BERT/Sarcasm/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/BERT/Sarcasm/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/BERT/Sarcasm/en-IN/\n",
            "   creating: Results/BERT/Sarcasm/en-IN/Reddit/\n",
            "  inflating: Results/BERT/Sarcasm/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/BERT/Sarcasm/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/BERT/Sarcasm/en-UK/\n",
            "   creating: Results/BERT/Sarcasm/en-UK/Reddit/\n",
            "  inflating: Results/BERT/Sarcasm/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/BERT/Sarcasm/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/BERT/Sentiment/\n",
            "   creating: Results/BERT/Sentiment/en-AU/\n",
            "   creating: Results/BERT/Sentiment/en-AU/Google/\n",
            "  inflating: Results/BERT/Sentiment/en-AU/Google/metric.csv  \n",
            "  inflating: Results/BERT/Sentiment/en-AU/Google/prediction.csv  \n",
            "   creating: Results/BERT/Sentiment/en-AU/Reddit/\n",
            "  inflating: Results/BERT/Sentiment/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/BERT/Sentiment/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/BERT/Sentiment/en-IN/\n",
            "   creating: Results/BERT/Sentiment/en-IN/Google/\n",
            "  inflating: Results/BERT/Sentiment/en-IN/Google/metric.csv  \n",
            "  inflating: Results/BERT/Sentiment/en-IN/Google/prediction.csv  \n",
            "   creating: Results/BERT/Sentiment/en-IN/Reddit/\n",
            "  inflating: Results/BERT/Sentiment/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/BERT/Sentiment/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/BERT/Sentiment/en-UK/\n",
            "   creating: Results/BERT/Sentiment/en-UK/Google/\n",
            "  inflating: Results/BERT/Sentiment/en-UK/Google/metric.csv  \n",
            "  inflating: Results/BERT/Sentiment/en-UK/Google/prediction.csv  \n",
            "   creating: Results/BERT/Sentiment/en-UK/Reddit/\n",
            "  inflating: Results/BERT/Sentiment/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/BERT/Sentiment/en-UK/Reddit/prediction.csv  \n",
            "  inflating: Results/Combined.csv    \n",
            "   creating: Results/distilbert-base-multilingual-cased/\n",
            "   creating: Results/distilbert-base-multilingual-cased/Sarcasm/\n",
            "   creating: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/\n",
            "   creating: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/.ipynb_checkpoints/\n",
            "   creating: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/CM_translated_train/\n",
            "  inflating: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/CM_translated_train/metric.csv  \n",
            "  inflating: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/CM_translated_train/prediction.csv  \n",
            "   creating: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/train_cm/\n",
            "  inflating: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/train_cm/metric.csv  \n",
            "  inflating: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/train_cm/prediction.csv  \n",
            "   creating: Results/distilbert-base-multilingual-cased/Sentiment/\n",
            "   creating: Results/distilbert-base-multilingual-cased/Sentiment/.ipynb_checkpoints/\n",
            "   creating: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/\n",
            "   creating: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/.ipynb_checkpoints/\n",
            "   creating: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/CM_translated_train/\n",
            "  inflating: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/CM_translated_train/metric.csv  \n",
            "  inflating: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/CM_translated_train/prediction.csv  \n",
            "   creating: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/train_cm/\n",
            "  inflating: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/train_cm/metric.csv  \n",
            "  inflating: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/train_cm/prediction.csv  \n",
            "   creating: Results/Gemma/\n",
            "   creating: Results/Gemma/Sarcasm/\n",
            "   creating: Results/Gemma/Sarcasm/en-AU/\n",
            "   creating: Results/Gemma/Sarcasm/en-AU/Reddit/\n",
            "  inflating: Results/Gemma/Sarcasm/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/Gemma/Sarcasm/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/Gemma/Sarcasm/en-IN/\n",
            "   creating: Results/Gemma/Sarcasm/en-IN/Reddit/\n",
            "  inflating: Results/Gemma/Sarcasm/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/Gemma/Sarcasm/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/Gemma/Sarcasm/en-UK/\n",
            "   creating: Results/Gemma/Sarcasm/en-UK/Reddit/\n",
            "  inflating: Results/Gemma/Sarcasm/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/Gemma/Sarcasm/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/Gemma/Sentiment/\n",
            "   creating: Results/Gemma/Sentiment/en-AU/\n",
            "   creating: Results/Gemma/Sentiment/en-AU/Google/\n",
            "  inflating: Results/Gemma/Sentiment/en-AU/Google/metric.csv  \n",
            "  inflating: Results/Gemma/Sentiment/en-AU/Google/prediction.csv  \n",
            "   creating: Results/Gemma/Sentiment/en-AU/Reddit/\n",
            "  inflating: Results/Gemma/Sentiment/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/Gemma/Sentiment/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/Gemma/Sentiment/en-IN/\n",
            "   creating: Results/Gemma/Sentiment/en-IN/Google/\n",
            "  inflating: Results/Gemma/Sentiment/en-IN/Google/metric.csv  \n",
            "  inflating: Results/Gemma/Sentiment/en-IN/Google/prediction.csv  \n",
            "   creating: Results/Gemma/Sentiment/en-IN/Reddit/\n",
            "  inflating: Results/Gemma/Sentiment/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/Gemma/Sentiment/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/Gemma/Sentiment/en-UK/\n",
            "   creating: Results/Gemma/Sentiment/en-UK/Google/\n",
            "  inflating: Results/Gemma/Sentiment/en-UK/Google/metric.csv  \n",
            "  inflating: Results/Gemma/Sentiment/en-UK/Google/prediction.csv  \n",
            "   creating: Results/Gemma/Sentiment/en-UK/Reddit/\n",
            "  inflating: Results/Gemma/Sentiment/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/Gemma/Sentiment/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/mDistilBERT/\n",
            "   creating: Results/mDistilBERT/Sarcasm/\n",
            "   creating: Results/mDistilBERT/Sarcasm/en-AU/\n",
            "   creating: Results/mDistilBERT/Sarcasm/en-AU/Reddit/\n",
            "  inflating: Results/mDistilBERT/Sarcasm/en-AU/Reddit/metric.csv  \n",
            "   creating: Results/mDistilBERT/Sarcasm/en-IN/\n",
            "   creating: Results/mDistilBERT/Sarcasm/en-IN/Reddit/\n",
            "  inflating: Results/mDistilBERT/Sarcasm/en-IN/Reddit/metric.csv  \n",
            "   creating: Results/mDistilBERT/Sarcasm/en-UK/\n",
            "   creating: Results/mDistilBERT/Sarcasm/en-UK/Reddit/\n",
            "  inflating: Results/mDistilBERT/Sarcasm/en-UK/Reddit/metric.csv  \n",
            "   creating: Results/mDistilBERT/Sentiment/\n",
            "   creating: Results/mDistilBERT/Sentiment/en-AU/\n",
            "   creating: Results/mDistilBERT/Sentiment/en-AU/Google/\n",
            "  inflating: Results/mDistilBERT/Sentiment/en-AU/Google/metric.csv  \n",
            "   creating: Results/mDistilBERT/Sentiment/en-AU/Reddit/\n",
            "  inflating: Results/mDistilBERT/Sentiment/en-AU/Reddit/metric.csv  \n",
            "   creating: Results/mDistilBERT/Sentiment/en-IN/\n",
            "   creating: Results/mDistilBERT/Sentiment/en-IN/Google/\n",
            "  inflating: Results/mDistilBERT/Sentiment/en-IN/Google/metric.csv  \n",
            "   creating: Results/mDistilBERT/Sentiment/en-IN/Reddit/\n",
            "  inflating: Results/mDistilBERT/Sentiment/en-IN/Reddit/metric.csv  \n",
            "   creating: Results/mDistilBERT/Sentiment/en-UK/\n",
            "   creating: Results/mDistilBERT/Sentiment/en-UK/Google/\n",
            "  inflating: Results/mDistilBERT/Sentiment/en-UK/Google/metric.csv  \n",
            "   creating: Results/mDistilBERT/Sentiment/en-UK/Reddit/\n",
            "  inflating: Results/mDistilBERT/Sentiment/en-UK/Reddit/metric.csv  \n",
            "   creating: Results/Mistral/\n",
            "   creating: Results/Mistral/Sarcasm/\n",
            "   creating: Results/Mistral/Sarcasm/en-AU/\n",
            "   creating: Results/Mistral/Sarcasm/en-AU/Reddit/\n",
            "  inflating: Results/Mistral/Sarcasm/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/Mistral/Sarcasm/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/Mistral/Sarcasm/en-IN/\n",
            "   creating: Results/Mistral/Sarcasm/en-IN/Reddit/\n",
            "  inflating: Results/Mistral/Sarcasm/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/Mistral/Sarcasm/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/Mistral/Sarcasm/en-UK/\n",
            "   creating: Results/Mistral/Sarcasm/en-UK/Reddit/\n",
            "  inflating: Results/Mistral/Sarcasm/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/Mistral/Sarcasm/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/Mistral/Sentiment/\n",
            "   creating: Results/Mistral/Sentiment/en-AU/\n",
            "   creating: Results/Mistral/Sentiment/en-AU/Google/\n",
            "  inflating: Results/Mistral/Sentiment/en-AU/Google/metric.csv  \n",
            "  inflating: Results/Mistral/Sentiment/en-AU/Google/prediction.csv  \n",
            "   creating: Results/Mistral/Sentiment/en-AU/Reddit/\n",
            "  inflating: Results/Mistral/Sentiment/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/Mistral/Sentiment/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/Mistral/Sentiment/en-IN/\n",
            "   creating: Results/Mistral/Sentiment/en-IN/Google/\n",
            "  inflating: Results/Mistral/Sentiment/en-IN/Google/metric.csv  \n",
            "  inflating: Results/Mistral/Sentiment/en-IN/Google/prediction.csv  \n",
            "   creating: Results/Mistral/Sentiment/en-IN/Reddit/\n",
            "  inflating: Results/Mistral/Sentiment/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/Mistral/Sentiment/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/Mistral/Sentiment/en-UK/\n",
            "   creating: Results/Mistral/Sentiment/en-UK/Google/\n",
            "  inflating: Results/Mistral/Sentiment/en-UK/Google/metric.csv  \n",
            "  inflating: Results/Mistral/Sentiment/en-UK/Google/prediction.csv  \n",
            "   creating: Results/Mistral/Sentiment/en-UK/Reddit/\n",
            "  inflating: Results/Mistral/Sentiment/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/Mistral/Sentiment/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/Qwen/\n",
            "   creating: Results/Qwen/Sarcasm/\n",
            "   creating: Results/Qwen/Sarcasm/en-AU/\n",
            "   creating: Results/Qwen/Sarcasm/en-AU/Reddit/\n",
            "  inflating: Results/Qwen/Sarcasm/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/Qwen/Sarcasm/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/Qwen/Sarcasm/en-IN/\n",
            "   creating: Results/Qwen/Sarcasm/en-IN/Reddit/\n",
            "  inflating: Results/Qwen/Sarcasm/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/Qwen/Sarcasm/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/Qwen/Sarcasm/en-UK/\n",
            "   creating: Results/Qwen/Sarcasm/en-UK/Reddit/\n",
            "  inflating: Results/Qwen/Sarcasm/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/Qwen/Sarcasm/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/Qwen/Sentiment/\n",
            "   creating: Results/Qwen/Sentiment/en-AU/\n",
            "   creating: Results/Qwen/Sentiment/en-AU/Google/\n",
            "  inflating: Results/Qwen/Sentiment/en-AU/Google/metric.csv  \n",
            "  inflating: Results/Qwen/Sentiment/en-AU/Google/prediction.csv  \n",
            "   creating: Results/Qwen/Sentiment/en-AU/Reddit/\n",
            "  inflating: Results/Qwen/Sentiment/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/Qwen/Sentiment/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/Qwen/Sentiment/en-IN/\n",
            "   creating: Results/Qwen/Sentiment/en-IN/Google/\n",
            "  inflating: Results/Qwen/Sentiment/en-IN/Google/metric.csv  \n",
            "  inflating: Results/Qwen/Sentiment/en-IN/Google/prediction.csv  \n",
            "   creating: Results/Qwen/Sentiment/en-IN/Reddit/\n",
            "  inflating: Results/Qwen/Sentiment/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/Qwen/Sentiment/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/Qwen/Sentiment/en-UK/\n",
            "   creating: Results/Qwen/Sentiment/en-UK/Google/\n",
            "  inflating: Results/Qwen/Sentiment/en-UK/Google/metric.csv  \n",
            "  inflating: Results/Qwen/Sentiment/en-UK/Google/prediction.csv  \n",
            "   creating: Results/Qwen/Sentiment/en-UK/Reddit/\n",
            "  inflating: Results/Qwen/Sentiment/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/Qwen/Sentiment/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/RoBERTa/\n",
            "   creating: Results/RoBERTa/Sarcasm/\n",
            "   creating: Results/RoBERTa/Sarcasm/en-AU/\n",
            "   creating: Results/RoBERTa/Sarcasm/en-AU/Reddit/\n",
            "  inflating: Results/RoBERTa/Sarcasm/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/RoBERTa/Sarcasm/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/RoBERTa/Sarcasm/en-IN/\n",
            "   creating: Results/RoBERTa/Sarcasm/en-IN/Reddit/\n",
            "  inflating: Results/RoBERTa/Sarcasm/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/RoBERTa/Sarcasm/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/RoBERTa/Sarcasm/en-UK/\n",
            "   creating: Results/RoBERTa/Sarcasm/en-UK/Reddit/\n",
            "  inflating: Results/RoBERTa/Sarcasm/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/RoBERTa/Sarcasm/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/RoBERTa/Sentiment/\n",
            "   creating: Results/RoBERTa/Sentiment/en-AU/\n",
            "   creating: Results/RoBERTa/Sentiment/en-AU/Google/\n",
            "  inflating: Results/RoBERTa/Sentiment/en-AU/Google/metric.csv  \n",
            "  inflating: Results/RoBERTa/Sentiment/en-AU/Google/prediction.csv  \n",
            "   creating: Results/RoBERTa/Sentiment/en-AU/Reddit/\n",
            "  inflating: Results/RoBERTa/Sentiment/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/RoBERTa/Sentiment/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/RoBERTa/Sentiment/en-IN/\n",
            "   creating: Results/RoBERTa/Sentiment/en-IN/Google/\n",
            "  inflating: Results/RoBERTa/Sentiment/en-IN/Google/metric.csv  \n",
            "  inflating: Results/RoBERTa/Sentiment/en-IN/Google/prediction.csv  \n",
            "   creating: Results/RoBERTa/Sentiment/en-IN/Reddit/\n",
            "  inflating: Results/RoBERTa/Sentiment/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/RoBERTa/Sentiment/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/RoBERTa/Sentiment/en-UK/\n",
            "   creating: Results/RoBERTa/Sentiment/en-UK/Google/\n",
            "  inflating: Results/RoBERTa/Sentiment/en-UK/Google/metric.csv  \n",
            "  inflating: Results/RoBERTa/Sentiment/en-UK/Google/prediction.csv  \n",
            "   creating: Results/RoBERTa/Sentiment/en-UK/Reddit/\n",
            "  inflating: Results/RoBERTa/Sentiment/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/RoBERTa/Sentiment/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/XLM-RoBERTa/\n",
            "   creating: Results/XLM-RoBERTa/Sarcasm/\n",
            "   creating: Results/XLM-RoBERTa/Sarcasm/en-AU/\n",
            "   creating: Results/XLM-RoBERTa/Sarcasm/en-AU/Reddit/\n",
            "  inflating: Results/XLM-RoBERTa/Sarcasm/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/XLM-RoBERTa/Sarcasm/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/XLM-RoBERTa/Sarcasm/en-IN/\n",
            "   creating: Results/XLM-RoBERTa/Sarcasm/en-IN/Reddit/\n",
            "  inflating: Results/XLM-RoBERTa/Sarcasm/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/XLM-RoBERTa/Sarcasm/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/XLM-RoBERTa/Sarcasm/en-UK/\n",
            "   creating: Results/XLM-RoBERTa/Sarcasm/en-UK/Reddit/\n",
            "  inflating: Results/XLM-RoBERTa/Sarcasm/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/XLM-RoBERTa/Sarcasm/en-UK/Reddit/prediction.csv  \n",
            "   creating: Results/XLM-RoBERTa/Sentiment/\n",
            "   creating: Results/XLM-RoBERTa/Sentiment/en-AU/\n",
            "   creating: Results/XLM-RoBERTa/Sentiment/en-AU/Google/\n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-AU/Google/metric.csv  \n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-AU/Google/prediction.csv  \n",
            "   creating: Results/XLM-RoBERTa/Sentiment/en-AU/Reddit/\n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-AU/Reddit/metric.csv  \n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-AU/Reddit/prediction.csv  \n",
            "   creating: Results/XLM-RoBERTa/Sentiment/en-IN/\n",
            "   creating: Results/XLM-RoBERTa/Sentiment/en-IN/Google/\n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-IN/Google/metric.csv  \n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-IN/Google/prediction.csv  \n",
            "   creating: Results/XLM-RoBERTa/Sentiment/en-IN/Reddit/\n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-IN/Reddit/metric.csv  \n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-IN/Reddit/prediction.csv  \n",
            "   creating: Results/XLM-RoBERTa/Sentiment/en-UK/\n",
            "   creating: Results/XLM-RoBERTa/Sentiment/en-UK/Google/\n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-UK/Google/metric.csv  \n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-UK/Google/prediction.csv  \n",
            "   creating: Results/XLM-RoBERTa/Sentiment/en-UK/Reddit/\n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-UK/Reddit/metric.csv  \n",
            "  inflating: Results/XLM-RoBERTa/Sentiment/en-UK/Reddit/prediction.csv  \n",
            "   creating: Utilities/\n",
            "   creating: Utilities/Cities/\n",
            "  inflating: Utilities/Cities/en-AU.csv  \n",
            "  inflating: Utilities/Cities/en-IN.csv  \n",
            "  inflating: Utilities/Cities/en-UK.csv  \n",
            "  inflating: Utilities/Cities/world_cities.csv  \n",
            "  inflating: Utilities/place_types.json  \n",
            "  inflating: Utilities/subreddits.json  \n",
            "  inflating: README.md               \n",
            "  inflating: requirements.txt        \n",
            "   creating: Code/\n",
            "   creating: Code/api/\n",
            "  inflating: Code/api/google_places.py  \n",
            "  inflating: Code/api/reddit_threads.py  \n",
            " extracting: Code/api/__init__.py    \n",
            "  inflating: Code/benchmark.py       \n",
            "   creating: Code/data_processing/\n",
            "  inflating: Code/data_processing/data_processing.py  \n",
            " extracting: Code/data_processing/__init__.py  \n",
            "   creating: Code/utils/\n",
            "  inflating: Code/utils/arguments.py  \n",
            "  inflating: Code/utils/cities.py    \n",
            "  inflating: Code/utils/data_util.py  \n",
            "  inflating: Code/utils/loss_util.py  \n",
            " extracting: Code/utils/__init__.py  \n",
            "   creating: Code/utils/__pycache__/\n",
            "  inflating: Code/utils/__pycache__/arguments.cpython-312.pyc  \n",
            "  inflating: Code/utils/__pycache__/data_util.cpython-312.pyc  \n",
            "  inflating: Code/utils/__pycache__/loss_util.cpython-312.pyc  \n",
            "  inflating: Code/utils/__pycache__/__init__.cpython-312.pyc  \n",
            "   creating: Dataset/\n",
            "   creating: Dataset/Splits/\n",
            "   creating: Dataset/Splits/Sarcasm/\n",
            "   creating: Dataset/Splits/Sarcasm/en-IN/\n",
            "   creating: Dataset/Splits/Sarcasm/en-IN/Reddit/\n",
            "   creating: Dataset/Splits/Sarcasm/en-IN/Reddit/all_translated/\n",
            "  inflating: Dataset/Splits/Sarcasm/en-IN/Reddit/all_translated/test_translated.csv  \n",
            "  inflating: Dataset/Splits/Sarcasm/en-IN/Reddit/all_translated/train_translated.csv  \n",
            "  inflating: Dataset/Splits/Sarcasm/en-IN/Reddit/all_translated/valid_translated.csv  \n",
            "   creating: Dataset/Splits/Sarcasm/en-IN/Reddit/Baseline/\n",
            "  inflating: Dataset/Splits/Sarcasm/en-IN/Reddit/Baseline/test_cm.csv  \n",
            "  inflating: Dataset/Splits/Sarcasm/en-IN/Reddit/Baseline/train_cm.csv  \n",
            "  inflating: Dataset/Splits/Sarcasm/en-IN/Reddit/Baseline/valid_cm.csv  \n",
            "   creating: Dataset/Splits/Sarcasm/en-IN/Reddit/CM_translated/\n",
            "  inflating: Dataset/Splits/Sarcasm/en-IN/Reddit/CM_translated/CM_translated_test.csv  \n",
            "  inflating: Dataset/Splits/Sarcasm/en-IN/Reddit/CM_translated/CM_translated_train.csv  \n",
            "  inflating: Dataset/Splits/Sarcasm/en-IN/Reddit/CM_translated/CM_translated_valid.csv  \n",
            "   creating: Dataset/Splits/Sentiment/\n",
            "   creating: Dataset/Splits/Sentiment/en-IN/\n",
            "   creating: Dataset/Splits/Sentiment/en-IN/Reddit/\n",
            "   creating: Dataset/Splits/Sentiment/en-IN/Reddit/all_translated/\n",
            "  inflating: Dataset/Splits/Sentiment/en-IN/Reddit/all_translated/translated_test_tagged_cm_processed.xlsx  \n",
            "  inflating: Dataset/Splits/Sentiment/en-IN/Reddit/all_translated/translated_train_tagged_cm_processed.xlsx  \n",
            "  inflating: Dataset/Splits/Sentiment/en-IN/Reddit/all_translated/translated_valid_tagged_cm_processed.xlsx  \n",
            "   creating: Dataset/Splits/Sentiment/en-IN/Reddit/Baseline/\n",
            "  inflating: Dataset/Splits/Sentiment/en-IN/Reddit/Baseline/test_cm.csv  \n",
            "  inflating: Dataset/Splits/Sentiment/en-IN/Reddit/Baseline/train_cm.csv  \n",
            "  inflating: Dataset/Splits/Sentiment/en-IN/Reddit/Baseline/valid_cm.csv  \n",
            "   creating: Dataset/Splits/Sentiment/en-IN/Reddit/CM_translated/\n",
            "  inflating: Dataset/Splits/Sentiment/en-IN/Reddit/CM_translated/CM_translated_test.csv  \n",
            "  inflating: Dataset/Splits/Sentiment/en-IN/Reddit/CM_translated/CM_translated_train.csv  \n",
            "  inflating: Dataset/Splits/Sentiment/en-IN/Reddit/CM_translated/CM_translated_valid.csv  \n",
            "   creating: ExampleConfigs/\n",
            "   creating: ExampleConfigs/Baselines/\n",
            "  inflating: ExampleConfigs/Baselines/en-IN-Reddit-Sarcasm-Classification-Baseline-bert.json  \n",
            "  inflating: ExampleConfigs/Baselines/en-IN-Reddit-Sarcasm-Classification-Baseline.json  \n",
            "  inflating: ExampleConfigs/Baselines/en-IN-Reddit-Sentiment-Classification-Baseline.json  \n",
            "   creating: ExampleConfigs/CM_translated/\n",
            "  inflating: ExampleConfigs/CM_translated/en-IN-Reddit-Sarcasm-Classification-cm-translated-bert.json  \n",
            "  inflating: ExampleConfigs/CM_translated/en-IN-Reddit-Sarcasm-Classification-cm-translated.json  \n",
            "  inflating: ExampleConfigs/CM_translated/en-IN-Reddit-Sentiment-Classification-cm-translated.json  \n",
            "  inflating: ExampleConfigs/en-AU-Google-Sentiment-Classification.json  \n",
            "  inflating: ExampleConfigs/en-UK-Reddit-Sentiment-Classification.json  \n",
            "   creating: Figures/\n",
            "  inflating: Figures/bench.png       \n",
            "  inflating: Figures/compare.png     \n",
            "  inflating: Figures/cross-bert.png  \n",
            "  inflating: Figures/cross-domain.png  \n",
            "  inflating: Figures/cross-mistral.png  \n",
            "  inflating: Figures/manual-variety-id.png  \n",
            "  inflating: Figures/sarc-all.png    \n",
            "  inflating: Figures/sent-all.png    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv_CCU9GQw-k",
        "outputId": "676cf15f-b5d4-4a3c-87fb-2bf2cfb9b6c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (1.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (26.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (0.21.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.7.0->evaluate) (8.3.1)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy, emoji, evaluate\n",
            "Successfully installed emoji-2.15.0 evaluate-0.4.6 ftfy-6.3.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# !pip install -r requirements.txt\n",
        "!pip install pydantic evaluate emoji ftfy"
      ],
      "id": "gv_CCU9GQw-k"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the core libraries for LLM fine-tuning\n",
        "!pip install transformers==4.41.2 peft==0.11.1 trl==0.9.4 bitsandbytes==0.42.0 accelerate==0.31.0 datasets==2.19.2\n",
        "\n",
        "# Install other utilities if missing\n",
        "!pip install scipy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-Ebzm8sQ_Ks",
        "outputId": "a8c1709c-963f-468f-b70f-619bf574de51"
      },
      "id": "a-Ebzm8sQ_Ks",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.41.2\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.11.1\n",
            "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl==0.9.4\n",
            "  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting bitsandbytes==0.42.0\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting accelerate==0.31.0\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting datasets==2.19.2\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (3.20.3)\n",
            "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers==4.41.2)\n",
            "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (2.32.4)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n",
            "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (4.67.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (2.9.0+cu128)\n",
            "Collecting tyro>=0.5.11 (from trl==0.9.4)\n",
            "  Downloading tyro-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.42.0) (1.16.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.19.2)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (0.70.16)\n",
            "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.2)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (3.13.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (1.22.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2) (2026.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.5.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.4) (0.17.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.4) (4.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.19.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.19.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.19.2) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.2) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.11.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (3.0.3)\n",
            "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.4-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-1.0.6-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: pyarrow-hotfix, fsspec, tyro, huggingface-hub, bitsandbytes, tokenizers, transformers, datasets, accelerate, trl, peft\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.4.0\n",
            "    Uninstalling huggingface_hub-1.4.0:\n",
            "      Successfully uninstalled huggingface_hub-1.4.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.2\n",
            "    Uninstalling tokenizers-0.22.2:\n",
            "      Successfully uninstalled tokenizers-0.22.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.18.1\n",
            "    Uninstalling peft-0.18.1:\n",
            "      Successfully uninstalled peft-0.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.31.0 bitsandbytes-0.42.0 datasets-2.19.2 fsspec-2024.3.1 huggingface-hub-0.36.2 peft-0.11.1 pyarrow-hotfix-0.7 tokenizers-0.19.1 transformers-4.41.2 trl-0.9.4 tyro-1.0.6\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpz55sO6IM2K",
        "outputId": "db9e3ff5-0afb-4ecf-d446-f7940263cb35"
      },
      "id": "fpz55sO6IM2K",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade bitsandbytes to the latest version\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSGrc8cDSJke",
        "outputId": "853e1c07-5421-414d-d71e-a941de7c794d"
      },
      "id": "jSGrc8cDSJke",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.42.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu128)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "  Attempting uninstall: bitsandbytes\n",
            "    Found existing installation: bitsandbytes 0.42.0\n",
            "    Uninstalling bitsandbytes-0.42.0:\n",
            "      Successfully uninstalled bitsandbytes-0.42.0\n",
            "Successfully installed bitsandbytes-0.49.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qANM8axcQw-m",
        "outputId": "480af10b-6ac0-4314-ed20-bf47cc9867bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-12 21:44:37.014832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770932677.035407    2142 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770932677.042466    2142 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770932677.060126    2142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770932677.060153    2142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770932677.060157    2142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770932677.060162    2142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-12 21:44:37.064671: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "02/12/2026 21:44:42 - INFO - __main__ - Random seed set to 50\n",
            "02/12/2026 21:44:42 - INFO - __main__ - Using device: cuda\n",
            "02/12/2026 21:44:42 - INFO - __main__ - Model name/path: distilbert/distilbert-base-multilingual-cased\n",
            "02/12/2026 21:44:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "tokenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 398kB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 466/466 [00:00<00:00, 4.29MB/s]\n",
            "vocab.txt: 996kB [00:00, 64.7MB/s]\n",
            "tokenizer.json: 1.96MB [00:00, 96.3MB/s]\n",
            "model.safetensors: 100% 542M/542M [00:10<00:00, 49.3MB/s]\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100% 1686/1686 [00:00<00:00, 5481.34 examples/s]\n",
            "Map: 100% 115/115 [00:00<00:00, 3990.74 examples/s]\n",
            "02/12/2026 21:44:56 - INFO - __main__ - Model loaded: distilbert\n",
            "02/12/2026 21:44:56 - INFO - __main__ - Model device: cuda:0\n",
            "02/12/2026 21:44:56 - INFO - __main__ - Positive weight: 3.763392857142857 | Negative weight: 0.5766073871409029\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "{'loss': 0.6902, 'grad_norm': 4.04522180557251, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.47}\n",
            "{'loss': 0.6491, 'grad_norm': 7.226452827453613, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.95}\n",
            "{'loss': 0.647, 'grad_norm': 16.055517196655273, 'learning_rate': 1.2e-05, 'epoch': 1.42}\n",
            "{'loss': 0.6929, 'grad_norm': 3.518165111541748, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.9}\n",
            "  6% 400/6330 [01:36<24:51,  3.98it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 19.71it/s]\u001b[A\n",
            " 33% 5/15 [00:00<00:00, 16.32it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 14.93it/s]\u001b[A\n",
            " 60% 9/15 [00:00<00:00, 14.22it/s]\u001b[A\n",
            " 73% 11/15 [00:00<00:00, 14.09it/s]\u001b[A\n",
            " 87% 13/15 [00:00<00:00, 13.90it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5809006690979004, 'eval_runtime': 1.0639, 'eval_samples_per_second': 108.092, 'eval_steps_per_second': 14.099, 'epoch': 1.9}\n",
            "  6% 400/6330 [01:38<24:51,  3.98it/s]\n",
            "100% 15/15 [00:00<00:00, 15.32it/s]\u001b[A\n",
            "{'loss': 0.5952, 'grad_norm': 6.8622589111328125, 'learning_rate': 2e-05, 'epoch': 2.37}\n",
            "{'loss': 0.6262, 'grad_norm': 7.8395562171936035, 'learning_rate': 1.965694682675815e-05, 'epoch': 2.84}\n",
            "{'loss': 0.4652, 'grad_norm': 14.569769859313965, 'learning_rate': 1.9313893653516296e-05, 'epoch': 3.32}\n",
            "{'loss': 0.4249, 'grad_norm': 1.5224546194076538, 'learning_rate': 1.8970840480274445e-05, 'epoch': 3.79}\n",
            " 13% 800/6330 [03:47<22:24,  4.11it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 20.49it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.30it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.46it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 15.06it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.68it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2853147983551025, 'eval_runtime': 1.0164, 'eval_samples_per_second': 113.15, 'eval_steps_per_second': 14.759, 'epoch': 3.79}\n",
            " 13% 800/6330 [03:48<22:24,  4.11it/s]\n",
            "100% 15/15 [00:00<00:00, 14.56it/s]\u001b[A\n",
            "{'loss': 0.2183, 'grad_norm': 42.021392822265625, 'learning_rate': 1.8627787307032593e-05, 'epoch': 4.27}\n",
            "{'loss': 0.1709, 'grad_norm': 2.8753156661987305, 'learning_rate': 1.828473413379074e-05, 'epoch': 4.74}\n",
            "{'loss': 0.0881, 'grad_norm': 0.025673147290945053, 'learning_rate': 1.7941680960548888e-05, 'epoch': 5.21}\n",
            "{'loss': 0.0709, 'grad_norm': 0.028659043833613396, 'learning_rate': 1.7598627787307033e-05, 'epoch': 5.69}\n",
            " 19% 1200/6330 [06:06<22:13,  3.85it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 20.37it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.31it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.51it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 14.96it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.63it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.3301339149475098, 'eval_runtime': 1.0271, 'eval_samples_per_second': 111.966, 'eval_steps_per_second': 14.604, 'epoch': 5.69}\n",
            " 19% 1200/6330 [06:07<22:13,  3.85it/s]\n",
            "100% 15/15 [00:00<00:00, 14.42it/s]\u001b[A\n",
            "{'loss': 0.0562, 'grad_norm': 0.013113103806972504, 'learning_rate': 1.7255574614065182e-05, 'epoch': 6.16}\n",
            "{'loss': 0.0596, 'grad_norm': 0.00923953577876091, 'learning_rate': 1.6912521440823327e-05, 'epoch': 6.64}\n",
            "{'loss': 0.0008, 'grad_norm': 0.008110550232231617, 'learning_rate': 1.6569468267581476e-05, 'epoch': 7.11}\n",
            "{'loss': 0.0006, 'grad_norm': 0.005583475343883038, 'learning_rate': 1.6226415094339625e-05, 'epoch': 7.58}\n",
            " 25% 1600/6330 [08:03<19:10,  4.11it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 20.48it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 15.82it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.04it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 14.57it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.23it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.871363639831543, 'eval_runtime': 1.0424, 'eval_samples_per_second': 110.32, 'eval_steps_per_second': 14.39, 'epoch': 7.58}\n",
            " 25% 1600/6330 [08:04<19:10,  4.11it/s]\n",
            "100% 15/15 [00:00<00:00, 14.04it/s]\u001b[A\n",
            "{'train_runtime': 521.2208, 'train_samples_per_second': 97.041, 'train_steps_per_second': 12.145, 'train_loss': 0.34100188709096985, 'epoch': 7.58}\n",
            " 25% 1600/6330 [08:41<25:40,  3.07it/s]\n",
            "Predicting: 100% 115/115 [00:01<00:00, 66.90it/s]\n",
            "02/12/2026 21:53:45 - INFO - __main__ - Predictions saved to Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/train_cm/prediction.csv\n",
            "02/12/2026 21:53:45 - INFO - __main__ - Metrics saved to Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/train_cm/metric.csv\n"
          ]
        }
      ],
      "source": [
        "# @title 2. Run Benchmark\n",
        "# Baseline Sarcasm\n",
        "!python Code/benchmark.py /content/ExampleConfigs/Baselines/en-IN-Reddit-Sarcasm-Classification-Baseline.json"
      ],
      "id": "qANM8axcQw-m"
    },
    {
      "cell_type": "code",
      "source": [
        "# cm translated sarcasm\n",
        "!python Code/benchmark.py /content/ExampleConfigs/CM_translated/en-IN-Reddit-Sarcasm-Classification-cm-translated.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDlEICjw0jju",
        "outputId": "5c04b16c-857e-4201-d800-d579fe2fadd9"
      },
      "id": "SDlEICjw0jju",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-12 21:54:17.676455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770933257.697118    4689 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770933257.703907    4689 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770933257.721513    4689 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770933257.721548    4689 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770933257.721556    4689 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770933257.721563    4689 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-12 21:54:17.726184: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "02/12/2026 21:54:23 - INFO - __main__ - Random seed set to 50\n",
            "02/12/2026 21:54:23 - INFO - __main__ - Using device: cuda\n",
            "02/12/2026 21:54:23 - INFO - __main__ - Model name/path: distilbert/distilbert-base-multilingual-cased\n",
            "02/12/2026 21:54:23 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100% 1686/1686 [00:00<00:00, 3509.30 examples/s]\n",
            "Map: 100% 115/115 [00:00<00:00, 2485.49 examples/s]\n",
            "02/12/2026 21:54:26 - INFO - __main__ - Model loaded: distilbert\n",
            "02/12/2026 21:54:26 - INFO - __main__ - Model device: cuda:0\n",
            "02/12/2026 21:54:26 - INFO - __main__ - Positive weight: 3.763392857142857 | Negative weight: 0.5766073871409029\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "{'loss': 0.6904, 'grad_norm': 4.1410956382751465, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.47}\n",
            "{'loss': 0.6598, 'grad_norm': 5.014279365539551, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.95}\n",
            "{'loss': 0.6392, 'grad_norm': 26.75905990600586, 'learning_rate': 1.2e-05, 'epoch': 1.42}\n",
            "{'loss': 0.7076, 'grad_norm': 3.760957717895508, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.9}\n",
            "  6% 400/6330 [01:39<23:58,  4.12it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 20.44it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.12it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.42it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 15.03it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.78it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6016272306442261, 'eval_runtime': 1.0186, 'eval_samples_per_second': 112.903, 'eval_steps_per_second': 14.727, 'epoch': 1.9}\n",
            "  6% 400/6330 [01:40<23:58,  4.12it/s]\n",
            "100% 15/15 [00:00<00:00, 14.48it/s]\u001b[A\n",
            "{'loss': 0.5955, 'grad_norm': 7.94901704788208, 'learning_rate': 2e-05, 'epoch': 2.37}\n",
            "{'loss': 0.6384, 'grad_norm': 9.184855461120605, 'learning_rate': 1.965694682675815e-05, 'epoch': 2.84}\n",
            "{'loss': 0.4527, 'grad_norm': 18.0198974609375, 'learning_rate': 1.9313893653516296e-05, 'epoch': 3.32}\n",
            "{'loss': 0.4353, 'grad_norm': 15.458855628967285, 'learning_rate': 1.8970840480274445e-05, 'epoch': 3.79}\n",
            " 13% 800/6330 [04:03<22:24,  4.11it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 20.82it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.29it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.46it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 15.01it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.66it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.5731773376464844, 'eval_runtime': 1.0159, 'eval_samples_per_second': 113.195, 'eval_steps_per_second': 14.765, 'epoch': 3.79}\n",
            " 13% 800/6330 [04:04<22:24,  4.11it/s]\n",
            "100% 15/15 [00:00<00:00, 14.48it/s]\u001b[A\n",
            "{'loss': 0.294, 'grad_norm': 70.26453399658203, 'learning_rate': 1.8627787307032593e-05, 'epoch': 4.27}\n",
            "{'loss': 0.3652, 'grad_norm': 0.4175654351711273, 'learning_rate': 1.828473413379074e-05, 'epoch': 4.74}\n",
            "{'loss': 0.1142, 'grad_norm': 0.047914598137140274, 'learning_rate': 1.7941680960548888e-05, 'epoch': 5.21}\n",
            "{'loss': 0.1826, 'grad_norm': 0.07317962497472763, 'learning_rate': 1.7598627787307033e-05, 'epoch': 5.69}\n",
            " 19% 1200/6330 [06:07<21:07,  4.05it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 20.66it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.19it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.40it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 14.94it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.79it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.8473842144012451, 'eval_runtime': 1.0179, 'eval_samples_per_second': 112.978, 'eval_steps_per_second': 14.736, 'epoch': 5.69}\n",
            " 19% 1200/6330 [06:08<21:07,  4.05it/s]\n",
            "100% 15/15 [00:00<00:00, 14.44it/s]\u001b[A\n",
            "{'loss': 0.0675, 'grad_norm': 0.02755132131278515, 'learning_rate': 1.7255574614065182e-05, 'epoch': 6.16}\n",
            "{'loss': 0.0487, 'grad_norm': 0.010128726251423359, 'learning_rate': 1.6912521440823327e-05, 'epoch': 6.64}\n",
            "{'loss': 0.0271, 'grad_norm': 0.008786623366177082, 'learning_rate': 1.6569468267581476e-05, 'epoch': 7.11}\n",
            "{'loss': 0.0057, 'grad_norm': 1.7268683910369873, 'learning_rate': 1.6226415094339625e-05, 'epoch': 7.58}\n",
            " 25% 1600/6330 [08:14<19:12,  4.10it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 21.53it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.50it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.55it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 14.67it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.50it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.914109945297241, 'eval_runtime': 1.0247, 'eval_samples_per_second': 112.233, 'eval_steps_per_second': 14.639, 'epoch': 7.58}\n",
            " 25% 1600/6330 [08:15<19:12,  4.10it/s]\n",
            "100% 15/15 [00:00<00:00, 14.34it/s]\u001b[A\n",
            "{'train_runtime': 518.7785, 'train_samples_per_second': 97.498, 'train_steps_per_second': 12.202, 'train_loss': 0.37024670127779247, 'epoch': 7.58}\n",
            " 25% 1600/6330 [08:38<25:33,  3.08it/s]\n",
            "Predicting: 100% 115/115 [00:01<00:00, 68.65it/s]\n",
            "02/12/2026 22:03:37 - INFO - __main__ - Predictions saved to Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/CM_translated_train/prediction.csv\n",
            "02/12/2026 22:03:37 - INFO - __main__ - Metrics saved to Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/CM_translated_train/metric.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentiment baseline\n",
        "!python Code/benchmark.py /content/ExampleConfigs/Baselines/en-IN-Reddit-Sentiment-Classification-Baseline.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFIDR2E44fbq",
        "outputId": "e83ad430-6459-423a-a998-5138abedca25"
      },
      "id": "VFIDR2E44fbq",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-12 22:04:49.029276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770933889.050723    7345 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770933889.057798    7345 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770933889.075554    7345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770933889.075580    7345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770933889.075584    7345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770933889.075589    7345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-12 22:04:49.080160: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "02/12/2026 22:04:54 - INFO - __main__ - Random seed set to 50\n",
            "02/12/2026 22:04:54 - INFO - __main__ - Using device: cuda\n",
            "02/12/2026 22:04:54 - INFO - __main__ - Model name/path: distilbert/distilbert-base-multilingual-cased\n",
            "02/12/2026 22:04:54 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100% 1685/1685 [00:00<00:00, 5352.96 examples/s]\n",
            "Map: 100% 115/115 [00:00<00:00, 5232.64 examples/s]\n",
            "02/12/2026 22:04:57 - INFO - __main__ - Model loaded: distilbert\n",
            "02/12/2026 22:04:58 - INFO - __main__ - Model device: cuda:0\n",
            "02/12/2026 22:04:58 - INFO - __main__ - Positive weight: 1.9638694638694638 | Negative weight: 0.67078025477707\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "{'loss': 0.6964, 'grad_norm': 3.7512731552124023, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.47}\n",
            "{'loss': 0.6846, 'grad_norm': 2.217758893966675, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.95}\n",
            "{'loss': 0.6397, 'grad_norm': 8.791501998901367, 'learning_rate': 1.2e-05, 'epoch': 1.42}\n",
            "{'loss': 0.5969, 'grad_norm': 9.172694206237793, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.9}\n",
            "  6% 400/6330 [01:38<24:05,  4.10it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 20.53it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.49it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.72it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 15.23it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.94it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.619242787361145, 'eval_runtime': 1.0028, 'eval_samples_per_second': 114.684, 'eval_steps_per_second': 14.959, 'epoch': 1.9}\n",
            "  6% 400/6330 [01:39<24:05,  4.10it/s]\n",
            "100% 15/15 [00:00<00:00, 14.81it/s]\u001b[A\n",
            "{'loss': 0.4882, 'grad_norm': 20.966217041015625, 'learning_rate': 2e-05, 'epoch': 2.37}\n",
            "{'loss': 0.4887, 'grad_norm': 11.02662467956543, 'learning_rate': 1.965694682675815e-05, 'epoch': 2.84}\n",
            "{'loss': 0.3624, 'grad_norm': 3.623216152191162, 'learning_rate': 1.9313893653516296e-05, 'epoch': 3.32}\n",
            "{'loss': 0.3573, 'grad_norm': 37.41077423095703, 'learning_rate': 1.8970840480274445e-05, 'epoch': 3.79}\n",
            " 13% 800/6330 [03:24<22:26,  4.11it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 21.27it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.43it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.52it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 15.06it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.60it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2365723848342896, 'eval_runtime': 1.0103, 'eval_samples_per_second': 113.826, 'eval_steps_per_second': 14.847, 'epoch': 3.79}\n",
            " 13% 800/6330 [03:25<22:26,  4.11it/s]\n",
            "100% 15/15 [00:00<00:00, 14.66it/s]\u001b[A\n",
            "{'loss': 0.2356, 'grad_norm': 19.068553924560547, 'learning_rate': 1.8627787307032593e-05, 'epoch': 4.27}\n",
            "{'loss': 0.1877, 'grad_norm': 0.0462963692843914, 'learning_rate': 1.828473413379074e-05, 'epoch': 4.74}\n",
            "{'loss': 0.1649, 'grad_norm': 0.025589345023036003, 'learning_rate': 1.7941680960548888e-05, 'epoch': 5.21}\n",
            "{'loss': 0.0726, 'grad_norm': 1.5067874193191528, 'learning_rate': 1.7598627787307033e-05, 'epoch': 5.69}\n",
            " 19% 1200/6330 [05:16<20:56,  4.08it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 20.77it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.34it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.51it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 14.87it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.3093395233154297, 'eval_runtime': 1.016, 'eval_samples_per_second': 113.187, 'eval_steps_per_second': 14.764, 'epoch': 5.69}\n",
            " 19% 1200/6330 [05:17<20:56,  4.08it/s]\n",
            "100% 15/15 [00:00<00:00, 14.47it/s]\u001b[A\n",
            "{'loss': 0.0855, 'grad_norm': 0.017362987622618675, 'learning_rate': 1.7255574614065182e-05, 'epoch': 6.16}\n",
            "{'loss': 0.0469, 'grad_norm': 0.016400478780269623, 'learning_rate': 1.6912521440823327e-05, 'epoch': 6.64}\n",
            "{'loss': 0.0558, 'grad_norm': 0.01739540882408619, 'learning_rate': 1.6569468267581476e-05, 'epoch': 7.11}\n",
            "{'loss': 0.0123, 'grad_norm': 0.0379934124648571, 'learning_rate': 1.6226415094339625e-05, 'epoch': 7.58}\n",
            " 25% 1600/6330 [07:11<19:14,  4.10it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 21.20it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.45it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.49it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 14.93it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.76it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.6828486919403076, 'eval_runtime': 1.0128, 'eval_samples_per_second': 113.55, 'eval_steps_per_second': 14.811, 'epoch': 7.58}\n",
            " 25% 1600/6330 [07:12<19:14,  4.10it/s]\n",
            "100% 15/15 [00:00<00:00, 14.59it/s]\u001b[A\n",
            "{'train_runtime': 457.292, 'train_samples_per_second': 110.542, 'train_steps_per_second': 13.842, 'train_loss': 0.32346056066453455, 'epoch': 7.58}\n",
            " 25% 1600/6330 [07:37<22:31,  3.50it/s]\n",
            "Predicting: 100% 115/115 [00:01<00:00, 65.96it/s]\n",
            "02/12/2026 22:12:45 - INFO - __main__ - Predictions saved to Results/distilbert-base-multilingual-cased/Sentiment/Reddit/train_cm/prediction.csv\n",
            "02/12/2026 22:12:45 - INFO - __main__ - Metrics saved to Results/distilbert-base-multilingual-cased/Sentiment/Reddit/train_cm/metric.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentiment cm translated\n",
        "!python Code/benchmark.py /content/ExampleConfigs/CM_translated/en-IN-Reddit-Sentiment-Classification-cm-translated.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBdBYIcy6i--",
        "outputId": "6cdcbebd-b610-40ce-f942-a4b0aafd5c4f"
      },
      "id": "eBdBYIcy6i--",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-12 22:13:58.832060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770934438.852987    9662 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770934438.860280    9662 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770934438.878404    9662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770934438.878433    9662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770934438.878438    9662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770934438.878443    9662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-12 22:13:58.883242: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "02/12/2026 22:14:05 - INFO - __main__ - Random seed set to 50\n",
            "02/12/2026 22:14:05 - INFO - __main__ - Using device: cuda\n",
            "02/12/2026 22:14:05 - INFO - __main__ - Model name/path: distilbert/distilbert-base-multilingual-cased\n",
            "02/12/2026 22:14:05 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100% 1685/1685 [00:00<00:00, 5361.03 examples/s]\n",
            "Map: 100% 115/115 [00:00<00:00, 5219.05 examples/s]\n",
            "02/12/2026 22:14:08 - INFO - __main__ - Model loaded: distilbert\n",
            "02/12/2026 22:14:08 - INFO - __main__ - Model device: cuda:0\n",
            "02/12/2026 22:14:08 - INFO - __main__ - Positive weight: 1.9638694638694638 | Negative weight: 0.67078025477707\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "{'loss': 0.6954, 'grad_norm': 3.760854959487915, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.47}\n",
            "{'loss': 0.68, 'grad_norm': 2.7137045860290527, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.95}\n",
            "{'loss': 0.6418, 'grad_norm': 7.980332851409912, 'learning_rate': 1.2e-05, 'epoch': 1.42}\n",
            "{'loss': 0.5907, 'grad_norm': 9.029593467712402, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.9}\n",
            "  6% 400/6330 [01:38<24:13,  4.08it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 19.93it/s]\u001b[A\n",
            " 33% 5/15 [00:00<00:00, 16.27it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 15.15it/s]\u001b[A\n",
            " 60% 9/15 [00:00<00:00, 14.68it/s]\u001b[A\n",
            " 73% 11/15 [00:00<00:00, 14.63it/s]\u001b[A\n",
            " 87% 13/15 [00:00<00:00, 14.42it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6224377751350403, 'eval_runtime': 1.0392, 'eval_samples_per_second': 110.661, 'eval_steps_per_second': 14.434, 'epoch': 1.9}\n",
            "  6% 400/6330 [01:39<24:13,  4.08it/s]\n",
            "100% 15/15 [00:00<00:00, 15.65it/s]\u001b[A\n",
            "{'loss': 0.4811, 'grad_norm': 21.52849006652832, 'learning_rate': 2e-05, 'epoch': 2.37}\n",
            "{'loss': 0.509, 'grad_norm': 10.780487060546875, 'learning_rate': 1.965694682675815e-05, 'epoch': 2.84}\n",
            "{'loss': 0.3675, 'grad_norm': 4.716073989868164, 'learning_rate': 1.9313893653516296e-05, 'epoch': 3.32}\n",
            "{'loss': 0.3579, 'grad_norm': 45.70155334472656, 'learning_rate': 1.8970840480274445e-05, 'epoch': 3.79}\n",
            " 13% 800/6330 [03:26<23:10,  3.98it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 19.24it/s]\u001b[A\n",
            " 33% 5/15 [00:00<00:00, 16.18it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 15.00it/s]\u001b[A\n",
            " 60% 9/15 [00:00<00:00, 14.32it/s]\u001b[A\n",
            " 73% 11/15 [00:00<00:00, 14.06it/s]\u001b[A\n",
            " 87% 13/15 [00:00<00:00, 13.81it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2520982027053833, 'eval_runtime': 1.0725, 'eval_samples_per_second': 107.228, 'eval_steps_per_second': 13.986, 'epoch': 3.79}\n",
            " 13% 800/6330 [03:27<23:10,  3.98it/s]\n",
            "100% 15/15 [00:01<00:00, 14.99it/s]\u001b[A\n",
            "{'loss': 0.2376, 'grad_norm': 1.4890034198760986, 'learning_rate': 1.8627787307032593e-05, 'epoch': 4.27}\n",
            "{'loss': 0.1363, 'grad_norm': 0.04074863716959953, 'learning_rate': 1.828473413379074e-05, 'epoch': 4.74}\n",
            "{'loss': 0.1261, 'grad_norm': 0.034041110426187515, 'learning_rate': 1.7941680960548888e-05, 'epoch': 5.21}\n",
            "{'loss': 0.0762, 'grad_norm': 18.427934646606445, 'learning_rate': 1.7598627787307033e-05, 'epoch': 5.69}\n",
            " 19% 1200/6330 [05:21<21:06,  4.05it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 21.29it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.46it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.45it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 15.02it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.79it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.9493224620819092, 'eval_runtime': 1.0165, 'eval_samples_per_second': 113.131, 'eval_steps_per_second': 14.756, 'epoch': 5.69}\n",
            " 19% 1200/6330 [05:22<21:06,  4.05it/s]\n",
            "100% 15/15 [00:00<00:00, 14.50it/s]\u001b[A\n",
            "{'loss': 0.0846, 'grad_norm': 0.019167272374033928, 'learning_rate': 1.7255574614065182e-05, 'epoch': 6.16}\n",
            "{'loss': 0.0345, 'grad_norm': 0.036201708018779755, 'learning_rate': 1.6912521440823327e-05, 'epoch': 6.64}\n",
            "{'loss': 0.0147, 'grad_norm': 0.01133937668055296, 'learning_rate': 1.6569468267581476e-05, 'epoch': 7.11}\n",
            "{'loss': 0.0179, 'grad_norm': 0.01178661733865738, 'learning_rate': 1.6226415094339625e-05, 'epoch': 7.58}\n",
            " 25% 1600/6330 [07:09<19:12,  4.10it/s]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 21.19it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 16.39it/s]\u001b[A\n",
            " 53% 8/15 [00:00<00:00, 15.23it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 14.88it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 14.61it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.539807081222534, 'eval_runtime': 1.0224, 'eval_samples_per_second': 112.476, 'eval_steps_per_second': 14.671, 'epoch': 7.58}\n",
            " 25% 1600/6330 [07:10<19:12,  4.10it/s]\n",
            "100% 15/15 [00:00<00:00, 14.48it/s]\u001b[A\n",
            "{'train_runtime': 444.7019, 'train_samples_per_second': 113.672, 'train_steps_per_second': 14.234, 'train_loss': 0.3157061499357223, 'epoch': 7.58}\n",
            " 25% 1600/6330 [07:24<21:54,  3.60it/s]\n",
            "Predicting: 100% 115/115 [00:01<00:00, 69.51it/s]\n",
            "02/12/2026 22:21:41 - INFO - __main__ - Predictions saved to Results/distilbert-base-multilingual-cased/Sentiment/Reddit/CM_translated_train/prediction.csv\n",
            "02/12/2026 22:21:41 - INFO - __main__ - Metrics saved to Results/distilbert-base-multilingual-cased/Sentiment/Reddit/CM_translated_train/metric.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3z0ggDW7Qw-m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "daddeaff-03df-4ab9-913c-60ad2b50725e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: Results/ (stored 0%)\n",
            "  adding: Results/ALBERT/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sarcasm/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-UK/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-UK/Reddit/metric.csv (deflated 5%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-UK/Reddit/prediction.csv (deflated 91%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-IN/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-IN/Reddit/metric.csv (deflated 5%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-IN/Reddit/prediction.csv (deflated 94%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-AU/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-AU/Reddit/metric.csv (deflated 5%)\n",
            "  adding: Results/ALBERT/Sarcasm/en-AU/Reddit/prediction.csv (deflated 89%)\n",
            "  adding: Results/ALBERT/Sentiment/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sentiment/en-UK/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sentiment/en-UK/Google/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sentiment/en-UK/Google/metric.csv (deflated 10%)\n",
            "  adding: Results/ALBERT/Sentiment/en-UK/Google/prediction.csv (deflated 90%)\n",
            "  adding: Results/ALBERT/Sentiment/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sentiment/en-UK/Reddit/metric.csv (deflated 15%)\n",
            "  adding: Results/ALBERT/Sentiment/en-UK/Reddit/prediction.csv (deflated 91%)\n",
            "  adding: Results/ALBERT/Sentiment/en-IN/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sentiment/en-IN/Google/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sentiment/en-IN/Google/metric.csv (deflated 5%)\n",
            "  adding: Results/ALBERT/Sentiment/en-IN/Google/prediction.csv (deflated 90%)\n",
            "  adding: Results/ALBERT/Sentiment/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sentiment/en-IN/Reddit/metric.csv (deflated 4%)\n",
            "  adding: Results/ALBERT/Sentiment/en-IN/Reddit/prediction.csv (deflated 89%)\n",
            "  adding: Results/ALBERT/Sentiment/en-AU/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sentiment/en-AU/Google/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sentiment/en-AU/Google/metric.csv (deflated 9%)\n",
            "  adding: Results/ALBERT/Sentiment/en-AU/Google/prediction.csv (deflated 89%)\n",
            "  adding: Results/ALBERT/Sentiment/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/ALBERT/Sentiment/en-AU/Reddit/metric.csv (deflated 7%)\n",
            "  adding: Results/ALBERT/Sentiment/en-AU/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/Qwen/ (stored 0%)\n",
            "  adding: Results/Qwen/Sarcasm/ (stored 0%)\n",
            "  adding: Results/Qwen/Sarcasm/en-UK/ (stored 0%)\n",
            "  adding: Results/Qwen/Sarcasm/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/Qwen/Sarcasm/en-UK/Reddit/metric.csv (deflated 5%)\n",
            "  adding: Results/Qwen/Sarcasm/en-UK/Reddit/prediction.csv (deflated 95%)\n",
            "  adding: Results/Qwen/Sarcasm/en-IN/ (stored 0%)\n",
            "  adding: Results/Qwen/Sarcasm/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/Qwen/Sarcasm/en-IN/Reddit/metric.csv (deflated 8%)\n",
            "  adding: Results/Qwen/Sarcasm/en-IN/Reddit/prediction.csv (deflated 95%)\n",
            "  adding: Results/Qwen/Sarcasm/en-AU/ (stored 0%)\n",
            "  adding: Results/Qwen/Sarcasm/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/Qwen/Sarcasm/en-AU/Reddit/metric.csv (deflated 20%)\n",
            "  adding: Results/Qwen/Sarcasm/en-AU/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/Qwen/Sentiment/ (stored 0%)\n",
            "  adding: Results/Qwen/Sentiment/en-UK/ (stored 0%)\n",
            "  adding: Results/Qwen/Sentiment/en-UK/Google/ (stored 0%)\n",
            "  adding: Results/Qwen/Sentiment/en-UK/Google/metric.csv (deflated 8%)\n",
            "  adding: Results/Qwen/Sentiment/en-UK/Google/prediction.csv (deflated 94%)\n",
            "  adding: Results/Qwen/Sentiment/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/Qwen/Sentiment/en-UK/Reddit/metric.csv (deflated 13%)\n",
            "  adding: Results/Qwen/Sentiment/en-UK/Reddit/prediction.csv (deflated 92%)\n",
            "  adding: Results/Qwen/Sentiment/en-IN/ (stored 0%)\n",
            "  adding: Results/Qwen/Sentiment/en-IN/Google/ (stored 0%)\n",
            "  adding: Results/Qwen/Sentiment/en-IN/Google/metric.csv (deflated 7%)\n",
            "  adding: Results/Qwen/Sentiment/en-IN/Google/prediction.csv (deflated 95%)\n",
            "  adding: Results/Qwen/Sentiment/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/Qwen/Sentiment/en-IN/Reddit/metric.csv (deflated 12%)\n",
            "  adding: Results/Qwen/Sentiment/en-IN/Reddit/prediction.csv (deflated 89%)\n",
            "  adding: Results/Qwen/Sentiment/en-AU/ (stored 0%)\n",
            "  adding: Results/Qwen/Sentiment/en-AU/Google/ (stored 0%)\n",
            "  adding: Results/Qwen/Sentiment/en-AU/Google/metric.csv (deflated 9%)\n",
            "  adding: Results/Qwen/Sentiment/en-AU/Google/prediction.csv (deflated 88%)\n",
            "  adding: Results/Qwen/Sentiment/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/Qwen/Sentiment/en-AU/Reddit/metric.csv (deflated 4%)\n",
            "  adding: Results/Qwen/Sentiment/en-AU/Reddit/prediction.csv (deflated 89%)\n",
            "  adding: Results/mDistilBERT/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sarcasm/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sarcasm/en-UK/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sarcasm/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sarcasm/en-UK/Reddit/metric.csv (deflated 16%)\n",
            "  adding: Results/mDistilBERT/Sarcasm/en-IN/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sarcasm/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sarcasm/en-IN/Reddit/metric.csv (deflated 4%)\n",
            "  adding: Results/mDistilBERT/Sarcasm/en-AU/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sarcasm/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sarcasm/en-AU/Reddit/metric.csv (deflated 10%)\n",
            "  adding: Results/mDistilBERT/Sentiment/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-UK/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-UK/Google/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-UK/Google/metric.csv (deflated 6%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-UK/Reddit/metric.csv (deflated 7%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-IN/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-IN/Google/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-IN/Google/metric.csv (deflated 8%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-IN/Reddit/metric.csv (deflated 7%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-AU/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-AU/Google/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-AU/Google/metric.csv (deflated 10%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/mDistilBERT/Sentiment/en-AU/Reddit/metric.csv (deflated 4%)\n",
            "  adding: Results/BERT/ (stored 0%)\n",
            "  adding: Results/BERT/Sarcasm/ (stored 0%)\n",
            "  adding: Results/BERT/Sarcasm/en-UK/ (stored 0%)\n",
            "  adding: Results/BERT/Sarcasm/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/BERT/Sarcasm/en-UK/Reddit/metric.csv (deflated 9%)\n",
            "  adding: Results/BERT/Sarcasm/en-UK/Reddit/prediction.csv (deflated 87%)\n",
            "  adding: Results/BERT/Sarcasm/en-IN/ (stored 0%)\n",
            "  adding: Results/BERT/Sarcasm/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/BERT/Sarcasm/en-IN/Reddit/metric.csv (deflated 5%)\n",
            "  adding: Results/BERT/Sarcasm/en-IN/Reddit/prediction.csv (deflated 94%)\n",
            "  adding: Results/BERT/Sarcasm/en-AU/ (stored 0%)\n",
            "  adding: Results/BERT/Sarcasm/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/BERT/Sarcasm/en-AU/Reddit/metric.csv (deflated 3%)\n",
            "  adding: Results/BERT/Sarcasm/en-AU/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/BERT/Sentiment/ (stored 0%)\n",
            "  adding: Results/BERT/Sentiment/en-UK/ (stored 0%)\n",
            "  adding: Results/BERT/Sentiment/en-UK/Google/ (stored 0%)\n",
            "  adding: Results/BERT/Sentiment/en-UK/Google/metric.csv (deflated 13%)\n",
            "  adding: Results/BERT/Sentiment/en-UK/Google/prediction.csv (deflated 90%)\n",
            "  adding: Results/BERT/Sentiment/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/BERT/Sentiment/en-UK/Reddit/metric.csv (deflated 34%)\n",
            "  adding: Results/BERT/Sentiment/en-UK/Reddit/prediction.csv (deflated 92%)\n",
            "  adding: Results/BERT/Sentiment/en-IN/ (stored 0%)\n",
            "  adding: Results/BERT/Sentiment/en-IN/Google/ (stored 0%)\n",
            "  adding: Results/BERT/Sentiment/en-IN/Google/metric.csv (deflated 7%)\n",
            "  adding: Results/BERT/Sentiment/en-IN/Google/prediction.csv (deflated 90%)\n",
            "  adding: Results/BERT/Sentiment/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/BERT/Sentiment/en-IN/Reddit/metric.csv (deflated 6%)\n",
            "  adding: Results/BERT/Sentiment/en-IN/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/BERT/Sentiment/en-AU/ (stored 0%)\n",
            "  adding: Results/BERT/Sentiment/en-AU/Google/ (stored 0%)\n",
            "  adding: Results/BERT/Sentiment/en-AU/Google/metric.csv (deflated 15%)\n",
            "  adding: Results/BERT/Sentiment/en-AU/Google/prediction.csv (deflated 89%)\n",
            "  adding: Results/BERT/Sentiment/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/BERT/Sentiment/en-AU/Reddit/metric.csv (deflated 16%)\n",
            "  adding: Results/BERT/Sentiment/en-AU/Reddit/prediction.csv (deflated 91%)\n",
            "  adding: Results/Gemma/ (stored 0%)\n",
            "  adding: Results/Gemma/Sarcasm/ (stored 0%)\n",
            "  adding: Results/Gemma/Sarcasm/en-UK/ (stored 0%)\n",
            "  adding: Results/Gemma/Sarcasm/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/Gemma/Sarcasm/en-UK/Reddit/metric.csv (deflated 8%)\n",
            "  adding: Results/Gemma/Sarcasm/en-UK/Reddit/prediction.csv (deflated 98%)\n",
            "  adding: Results/Gemma/Sarcasm/en-IN/ (stored 0%)\n",
            "  adding: Results/Gemma/Sarcasm/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/Gemma/Sarcasm/en-IN/Reddit/metric.csv (deflated 7%)\n",
            "  adding: Results/Gemma/Sarcasm/en-IN/Reddit/prediction.csv (deflated 97%)\n",
            "  adding: Results/Gemma/Sarcasm/en-AU/ (stored 0%)\n",
            "  adding: Results/Gemma/Sarcasm/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/Gemma/Sarcasm/en-AU/Reddit/metric.csv (deflated 2%)\n",
            "  adding: Results/Gemma/Sarcasm/en-AU/Reddit/prediction.csv (deflated 98%)\n",
            "  adding: Results/Gemma/Sentiment/ (stored 0%)\n",
            "  adding: Results/Gemma/Sentiment/en-UK/ (stored 0%)\n",
            "  adding: Results/Gemma/Sentiment/en-UK/Google/ (stored 0%)\n",
            "  adding: Results/Gemma/Sentiment/en-UK/Google/metric.csv (deflated 8%)\n",
            "  adding: Results/Gemma/Sentiment/en-UK/Google/prediction.csv (deflated 91%)\n",
            "  adding: Results/Gemma/Sentiment/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/Gemma/Sentiment/en-UK/Reddit/metric.csv (deflated 8%)\n",
            "  adding: Results/Gemma/Sentiment/en-UK/Reddit/prediction.csv (deflated 97%)\n",
            "  adding: Results/Gemma/Sentiment/en-IN/ (stored 0%)\n",
            "  adding: Results/Gemma/Sentiment/en-IN/Google/ (stored 0%)\n",
            "  adding: Results/Gemma/Sentiment/en-IN/Google/metric.csv (deflated 8%)\n",
            "  adding: Results/Gemma/Sentiment/en-IN/Google/prediction.csv (deflated 98%)\n",
            "  adding: Results/Gemma/Sentiment/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/Gemma/Sentiment/en-IN/Reddit/metric.csv (deflated 1%)\n",
            "  adding: Results/Gemma/Sentiment/en-IN/Reddit/prediction.csv (deflated 98%)\n",
            "  adding: Results/Gemma/Sentiment/en-AU/ (stored 0%)\n",
            "  adding: Results/Gemma/Sentiment/en-AU/Google/ (stored 0%)\n",
            "  adding: Results/Gemma/Sentiment/en-AU/Google/metric.csv (deflated 15%)\n",
            "  adding: Results/Gemma/Sentiment/en-AU/Google/prediction.csv (deflated 88%)\n",
            "  adding: Results/Gemma/Sentiment/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/Gemma/Sentiment/en-AU/Reddit/metric.csv (deflated 15%)\n",
            "  adding: Results/Gemma/Sentiment/en-AU/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/Mistral/ (stored 0%)\n",
            "  adding: Results/Mistral/Sarcasm/ (stored 0%)\n",
            "  adding: Results/Mistral/Sarcasm/en-UK/ (stored 0%)\n",
            "  adding: Results/Mistral/Sarcasm/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/Mistral/Sarcasm/en-UK/Reddit/metric.csv (deflated 11%)\n",
            "  adding: Results/Mistral/Sarcasm/en-UK/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/Mistral/Sarcasm/en-IN/ (stored 0%)\n",
            "  adding: Results/Mistral/Sarcasm/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/Mistral/Sarcasm/en-IN/Reddit/metric.csv (deflated 13%)\n",
            "  adding: Results/Mistral/Sarcasm/en-IN/Reddit/prediction.csv (deflated 97%)\n",
            "  adding: Results/Mistral/Sarcasm/en-AU/ (stored 0%)\n",
            "  adding: Results/Mistral/Sarcasm/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/Mistral/Sarcasm/en-AU/Reddit/metric.csv (deflated 11%)\n",
            "  adding: Results/Mistral/Sarcasm/en-AU/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/Mistral/Sentiment/ (stored 0%)\n",
            "  adding: Results/Mistral/Sentiment/en-UK/ (stored 0%)\n",
            "  adding: Results/Mistral/Sentiment/en-UK/Google/ (stored 0%)\n",
            "  adding: Results/Mistral/Sentiment/en-UK/Google/metric.csv (deflated 6%)\n",
            "  adding: Results/Mistral/Sentiment/en-UK/Google/prediction.csv (deflated 91%)\n",
            "  adding: Results/Mistral/Sentiment/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/Mistral/Sentiment/en-UK/Reddit/metric.csv (deflated 7%)\n",
            "  adding: Results/Mistral/Sentiment/en-UK/Reddit/prediction.csv (deflated 92%)\n",
            "  adding: Results/Mistral/Sentiment/en-IN/ (stored 0%)\n",
            "  adding: Results/Mistral/Sentiment/en-IN/Google/ (stored 0%)\n",
            "  adding: Results/Mistral/Sentiment/en-IN/Google/metric.csv (deflated 7%)\n",
            "  adding: Results/Mistral/Sentiment/en-IN/Google/prediction.csv (deflated 90%)\n",
            "  adding: Results/Mistral/Sentiment/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/Mistral/Sentiment/en-IN/Reddit/metric.csv (deflated 14%)\n",
            "  adding: Results/Mistral/Sentiment/en-IN/Reddit/prediction.csv (deflated 92%)\n",
            "  adding: Results/Mistral/Sentiment/en-AU/ (stored 0%)\n",
            "  adding: Results/Mistral/Sentiment/en-AU/Google/ (stored 0%)\n",
            "  adding: Results/Mistral/Sentiment/en-AU/Google/metric.csv (deflated 20%)\n",
            "  adding: Results/Mistral/Sentiment/en-AU/Google/prediction.csv (deflated 88%)\n",
            "  adding: Results/Mistral/Sentiment/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/Mistral/Sentiment/en-AU/Reddit/metric.csv (deflated 10%)\n",
            "  adding: Results/Mistral/Sentiment/en-AU/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/Combined.csv (deflated 67%)\n",
            "  adding: Results/XLM-RoBERTa/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-UK/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-UK/Reddit/metric.csv (deflated 10%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-UK/Reddit/prediction.csv (deflated 91%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-IN/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-IN/Reddit/metric.csv (deflated 11%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-IN/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-AU/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-AU/Reddit/metric.csv (deflated 6%)\n",
            "  adding: Results/XLM-RoBERTa/Sarcasm/en-AU/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-UK/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-UK/Google/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-UK/Google/metric.csv (deflated 16%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-UK/Google/prediction.csv (deflated 91%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-UK/Reddit/metric.csv (deflated 5%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-UK/Reddit/prediction.csv (deflated 93%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-IN/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-IN/Google/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-IN/Google/metric.csv (deflated 2%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-IN/Google/prediction.csv (deflated 98%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-IN/Reddit/metric.csv (deflated 1%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-IN/Reddit/prediction.csv (deflated 89%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-AU/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-AU/Google/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-AU/Google/metric.csv (deflated 10%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-AU/Google/prediction.csv (deflated 88%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-AU/Reddit/metric.csv (deflated 18%)\n",
            "  adding: Results/XLM-RoBERTa/Sentiment/en-AU/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/ (stored 0%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sarcasm/ (stored 0%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/ (stored 0%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/CM_translated_train/ (stored 0%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/CM_translated_train/metric.csv (deflated 31%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/CM_translated_train/prediction.csv (deflated 85%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/train_cm/ (stored 0%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/train_cm/metric.csv (deflated 4%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sarcasm/Reddit/train_cm/prediction.csv (deflated 85%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sentiment/ (stored 0%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/ (stored 0%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/CM_translated_train/ (stored 0%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/CM_translated_train/metric.csv (deflated 13%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/CM_translated_train/prediction.csv (deflated 76%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/train_cm/ (stored 0%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/train_cm/metric.csv (deflated 14%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sentiment/Reddit/train_cm/prediction.csv (deflated 78%)\n",
            "  adding: Results/distilbert-base-multilingual-cased/Sentiment/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: Results/RoBERTa/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sarcasm/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-UK/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-UK/Reddit/metric.csv (deflated 5%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-UK/Reddit/prediction.csv (deflated 87%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-IN/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-IN/Reddit/metric.csv (deflated 8%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-IN/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-AU/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-AU/Reddit/metric.csv (deflated 14%)\n",
            "  adding: Results/RoBERTa/Sarcasm/en-AU/Reddit/prediction.csv (deflated 92%)\n",
            "  adding: Results/RoBERTa/Sentiment/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-UK/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-UK/Google/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-UK/Google/metric.csv (deflated 15%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-UK/Google/prediction.csv (deflated 90%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-UK/Reddit/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-UK/Reddit/metric.csv (deflated 10%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-UK/Reddit/prediction.csv (deflated 91%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-IN/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-IN/Google/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-IN/Google/metric.csv (deflated 5%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-IN/Google/prediction.csv (deflated 91%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-IN/Reddit/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-IN/Reddit/metric.csv (deflated 7%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-IN/Reddit/prediction.csv (deflated 90%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-AU/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-AU/Google/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-AU/Google/metric.csv (deflated 11%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-AU/Google/prediction.csv (deflated 88%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-AU/Reddit/ (stored 0%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-AU/Reddit/metric.csv (deflated 2%)\n",
            "  adding: Results/RoBERTa/Sentiment/en-AU/Reddit/prediction.csv (deflated 91%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_34964ae1-7aa0-46d5-abf9-be377d663cbc\", \"results.zip\", 78455)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title 3. Download Results\n",
        "from google.colab import files\n",
        "!zip -r results.zip Results/\n",
        "files.download('results.zip')"
      ],
      "id": "3z0ggDW7Qw-m"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cr9NlXT38oHi"
      },
      "id": "cr9NlXT38oHi",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}